{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import helpers\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import multiprocessing as mp\n",
    "import os \n",
    "import math\n",
    "from functools import partial\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "# Load the data from files\n",
    "with open(f'{DATA_DIR}/corpus.jsonl', 'r') as f:\n",
    "    corpus_data = {int(item['_id']): item['text'] for item in (json.loads(line) for line in f)}\n",
    "\n",
    "with open(f'{DATA_DIR}/queries.jsonl', 'r') as f:\n",
    "    queries_data = {int(item['_id']): item['text'] for item in (json.loads(line) for line in f)}\n",
    "\n",
    "train_data = pd.read_csv(f'{DATA_DIR}/task1_train.tsv', delimiter='\\t')\n",
    "test_data = pd.read_csv(f'{DATA_DIR}/task1_test.tsv', delimiter='\\t')\n",
    "\n",
    "# Rename corpus-id to document_id and query-id to query_id in both train and test data\n",
    "train_data = train_data.rename(columns={'corpus-id': 'document_id', 'query-id': 'query_id'})\n",
    "test_data = test_data.rename(columns={'corpus-id': 'document_id', 'query-id': 'query_id'})\n",
    "# Make sure that the document_id and query_id are int64\n",
    "train_data['document_id'] = train_data['document_id'].astype('int64')\n",
    "train_data['query_id'] = train_data['query_id'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a df from the corpus data\n",
    "corpus_df = pd.DataFrame.from_dict(corpus_data, orient='index', columns=['text'])\n",
    "# Create a df from the queries data\n",
    "queries_df = pd.DataFrame.from_dict(queries_data, orient='index', columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = corpus_df['text'].tolist()\n",
    "queries = queries_df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_name = \"bert-base-uncased\"  # You can choose a specific BERT variant\n",
    "model = AutoModel.from_pretrained(model_name).to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be21dd83b4b3456ca0b374af8b7a7094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45982 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "import torch\n",
    "\n",
    "batch_size = 32\n",
    "# document_tokens = [tokenizer(doc, padding=True, truncation=True, return_tensors=\"pt\") for doc in tqdm(documents)]\n",
    "# create empty tensor\n",
    "embeddings = torch.zeros(len(documents), 768).to('cuda')\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(documents), batch_size)):\n",
    "        batch = documents[i:i+batch_size]\n",
    "        # encode\n",
    "        document_tokens = tokenizer.batch_encode_plus(batch, padding=True, truncation=True, return_tensors=\"pt\").to('cuda')\n",
    "        # document_tokens is of type BatchEncoding\n",
    "        outputs = model(input_ids=document_tokens['input_ids'], attention_mask=document_tokens['attention_mask'], token_type_ids=document_tokens['token_type_ids'])\n",
    "        outputs = outputs.last_hidden_state # shape of batchx?x768\n",
    "        # average pooling\n",
    "        outputs = torch.mean(outputs, dim=1)\n",
    "        # torch normalize\n",
    "        outputs = torch.nn.functional.normalize(outputs, p=2, dim=1)\n",
    "        # shape now batchx768\n",
    "        embeddings[i:i+batch_size] = outputs\n",
    "\n",
    "\n",
    "# save embeddings to file\n",
    "torch.save(embeddings, 'bert_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize embeddings\n",
    "embeddings = normalize(embeddings.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first query\n",
    "query = queries[0]\n",
    "query_tokens = tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed query\n",
    "query_embedding = model(**query_tokens).last_hidden_state.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarities = cosine_similarity(query_embedding, document_embeddings).flatten()\n",
    "top_indices = similarities.argsort()[::-1][:10]\n",
    "\n",
    "for index in top_indices:\n",
    "    print(f'Document id: {corpus_df.iloc[index].name}')\n",
    "    print(f'Document text: {corpus_df.iloc[index].text}')\n",
    "    print(f'Similarity: {similarities[index]}')\n",
    "    print('______________________________')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
