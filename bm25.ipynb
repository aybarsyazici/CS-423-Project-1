{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenized documents from pickle file...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import helpers\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "import multiprocessing as mp\n",
    "import os \n",
    "import math\n",
    "from functools import partial\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "# Load the data from files\n",
    "with open(f'{DATA_DIR}/corpus.jsonl', 'r') as f:\n",
    "    corpus_data = {int(item['_id']): item['text'] for item in (json.loads(line) for line in f)}\n",
    "\n",
    "with open(f'{DATA_DIR}/queries.jsonl', 'r') as f:\n",
    "    queries_data = {int(item['_id']): item['text'] for item in (json.loads(line) for line in f)}\n",
    "\n",
    "train_data = pd.read_csv(f'{DATA_DIR}/task1_train.tsv', delimiter='\\t')\n",
    "test_data = pd.read_csv(f'{DATA_DIR}/task1_test.tsv', delimiter='\\t')\n",
    "\n",
    "# Rename corpus-id to document_id and query-id to query_id in both train and test data\n",
    "train_data = train_data.rename(columns={'corpus-id': 'document_id', 'query-id': 'query_id'})\n",
    "test_data = test_data.rename(columns={'corpus-id': 'document_id', 'query-id': 'query_id'})\n",
    "# Make sure that the document_id and query_id are int64\n",
    "train_data['document_id'] = train_data['document_id'].astype('int64')\n",
    "train_data['query_id'] = train_data['query_id'].astype('int64')\n",
    "\n",
    "# Create a df from the corpus data\n",
    "corpus_df = pd.DataFrame.from_dict(corpus_data, orient='index', columns=['text'])\n",
    "# Create a df from the queries data\n",
    "queries_df = pd.DataFrame.from_dict(queries_data, orient='index', columns=['text'])\n",
    "\n",
    "# Check if documents.pkl exists:\n",
    "if os.path.isfile(f'{DATA_DIR}/documents.pkl'):\n",
    "    print('Loading tokenized documents from pickle file...')\n",
    "    # load the tokenized documents from pickle file\n",
    "    with open(f'{DATA_DIR}/documents.pkl', 'rb') as f:\n",
    "        documents = pickle.load(f)\n",
    "else:\n",
    "    print('File not found. Tokenizing documents...')\n",
    "    documents = corpus_df['text'].tolist()\n",
    "    documents = [x.strip() for x in documents]\n",
    "    # use multiprocessing to speed up the process\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    # pass both documents and stemmer as arguments to the tokenize function\n",
    "    fn = partial(helpers.tokenize, stemmer=stemmer)   \n",
    "    documents = list(tqdm(pool.imap(fn, documents), total=len(documents))) \n",
    "    # save the tokenized documents as pickle file\n",
    "    with open(f'{DATA_DIR}/documents.pkl', 'wb') as f:\n",
    "        pickle.dump(documents, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = list(set([item for sublist in documents for item in sublist]))\n",
    "vocabulary.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b803cc1de834cffb93a3b253ff9e8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1471406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute the number of documents that contain each word\n",
    "doc_freqs = {}\n",
    "for doc in tqdm(documents):\n",
    "    for word in set(doc):\n",
    "        doc_freqs[word] = doc_freqs.get(word, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the IDF for each word in the vocabulary\n",
    "num_docs = len(documents)\n",
    "#idfs = {word: math.log(num_docs / freq) for word, freq in doc_freqs.items()}\n",
    "idfs = {word: math.log((num_docs - freq + 0.5)/(freq + 0.5)+1) for word, freq in doc_freqs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8407aed36e974147b36d074e0e7052b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:01, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a vocabulary dictionary with the index of each word in the vocabulary\n",
    "vocabulary_dict = {word: i for i, word in tqdm(enumerate(vocabulary))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find average document length\n",
    "avg_doc_length = sum([len(doc) for doc in documents]) / len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get the first row of the query matrix\n",
    "query = queries_df['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the query\n",
    "query = helpers.tokenize(query, stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "858fdf6a2fc0478d9859a61e9ee4b10d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring documents...:   0%|          | 0/1471406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = []\n",
    "for doc in tqdm(documents, desc=\"Scoring documents...\"):\n",
    "    score = helpers.bm25(doc, query, idfs, avg_doc_length, k1=1, b=0.75)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the documents with the highest scores (top 10)\n",
    "top_10 = np.argsort(scores)[::-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 7306165\n",
      "Text: \"He is honest-but smart as hell.. Meanwhile, President Truman was told of the successful test of the Manhattan Project (atomic bomb) in Alamogordo, New Mexico on Jul 16, 1945. Diary of President Truman of Jul 18, 1945 shows Discussed Manhattan (it is a success).e is honest-but smart as hell.. Meanwhile, President Truman was told of the successful test of the Manhattan Project (atomic bomb) in Alamogordo, New Mexico on Jul 16, 1945. Diary of President Truman of Jul 18, 1945 shows Discussed Manhattan (it is a success).\"\n",
      "Score: 22.80581471839291\n",
      "\n",
      "Document ID: 0\n",
      "Text: \"The presence of communication amid scientific minds was equally important to the success of the Manhattan Project as scientific intellect was. The only cloud hanging over the impressive achievement of the atomic researchers and engineers is what their success truly meant; hundreds of thousands of innocent lives obliterated.\"\n",
      "Score: 19.472276735865158\n",
      "\n",
      "Document ID: 2036644\n",
      "Text: \"Manhattan Project. The Manhattan Project was a secret military project created in 1942 to produce the first US nuclear weapon. Fears that Nazi Germany would build and use a nuclear weapon during World War II triggered the start of the Manhattan Project, which was originally based in Manhattan, New York.anhattan Project. The Manhattan Project was a secret military project created in 1942 to produce the first US nuclear weapon. Fears that Nazi Germany would build and use a nuclear weapon during World War II triggered the start of the Manhattan Project, which was originally based in Manhattan, New York.\"\n",
      "Score: 19.16563056774753\n",
      "\n",
      "Document ID: 3870080\n",
      "Text: \"Manhattan Project. The Manhattan Project was a secret military project created in 1942 to produce the first US nuclear weapon. Fears that Nazi Germany would build and use a nuclear weapon during World War II triggered the start of the Manhattan Project, which was originally based in Manhattan, New York.he Manhattan Project was a secret military project created in 1942 to produce the first US nuclear weapon. Fears that Nazi Germany would build and use a nuclear weapon during World War II triggered the start of the Manhattan Project, which was originally based in Manhattan, New York.\"\n",
      "Score: 19.07084666529361\n",
      "\n",
      "Document ID: 3607205\n",
      "Text: \"Manhattan Project. 1  The Manhattan Project was a secret military project created in 1942 to produce the first US nuclear weapon. Fears that Nazi Germany would build and use a nuclear weapon during World War II triggered the start of the Manhattan Project, which was originally based in Manhattan, New York.\"\n",
      "Score: 18.84375329013862\n",
      "\n",
      "Document ID: 6434305\n",
      "Text: \"Beyond the Sweatshop: poverty footprints and supply chains. 1  Oxfam's Poverty Footprint project identifies five social impact metrics - building on the example of successful schemes measuring environmental impact.\"\n",
      "Score: 18.749677315523968\n",
      "\n",
      "Document ID: 7243450\n",
      "Text: \"The project was given its name due to the fact that at least 10 of the sites used for the research were located in Manhattan. Following is a timeline of the key events related to the development of the atomic bomb and the Manhattan Project. Manhattan Project Timeline\"\n",
      "Score: 18.63664236327419\n",
      "\n",
      "Document ID: 3870082\n",
      "Text: \"The Manhattan Project was a research and development project that produced the first nuclear weapons during World War II.he Army component of the project was designated the Manhattan District; Manhattan gradually superseded the official codename, Development of Substitute Materials, for the entire project. Along the way, the project absorbed its earlier British counterpart, Tube Alloys.\"\n",
      "Score: 18.276555075929313\n",
      "\n",
      "Document ID: 4138462\n",
      "Text: \"The Manhattan Project was a secret military project created in 1942 to produce the first US nuclear weapon. Fears that Nazi Germany would build and use a nuclear weapon during World War II triggered the start of the Manhattan Project, which was originally based in Manhattan, New York.\"\n",
      "Score: 17.946553484549558\n",
      "\n",
      "Document ID: 2395246\n",
      "Text: \"The Manhattan Project was a research and development project that produced the first atomic bombs during World War II. Here's an answer: The Manhattan Project was a movie made about Peter Stuyvesant's quest to buy the island of Manhattan from Native Americans back in the 1600s. For a better answer, look below.\"\n",
      "Score: 17.852119329582376\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index in top_10:\n",
    "    print(f'Document ID: {corpus_df.index.values[index]}')\n",
    "    print(f'Text: \"{corpus_df.iloc[index].text}\"')\n",
    "    print(f'Score: {scores[index]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading document matrix from pickle file...\n"
     ]
    }
   ],
   "source": [
    "# Since finding all the scores for each query takes a long time,\n",
    "# we will use matrix multiplication to find the scores for all the queries at once\n",
    "\n",
    "k1 = 1\n",
    "b = 0.75\n",
    "\n",
    "# first create the document matrix where each row is a document\n",
    "# and each column is a word\n",
    "# the value of the cell is idfs[term] * (counts[term] * (k1 + 1) / (counts[term] + k1 * (1 - b + b * len(doc) / avg_doc_length)))\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "# check if doc_matrix.pkl exists\n",
    "if os.path.isfile(f'{DATA_DIR}/doc_matrix.pkl'):\n",
    "    print('Loading document matrix from pickle file...')\n",
    "    # load the document matrix from pickle file\n",
    "    with open(f'{DATA_DIR}/doc_matrix.pkl', 'rb') as f:\n",
    "        doc_matrix = pickle.load(f)\n",
    "else:\n",
    "    # Compute the doc matrix\n",
    "    doc_matrix = lil_matrix((len(documents), len(vocabulary))) # We use lil_matrix since it is efficient in incremental assignments\n",
    "    for doc_id, doc in tqdm(enumerate(documents), desc=\"Computing document matrix\"):\n",
    "        counts, max_count = helpers.count_terms(doc)\n",
    "        for term, count in counts.items():\n",
    "            if term in vocabulary_dict:\n",
    "                term_id = vocabulary_dict[term]\n",
    "                doc_matrix[doc_id, term_id] = idfs[term] * (counts[term] * (k1 + 1) / (counts[term] + k1 * (1 - b + b * len(doc) / avg_doc_length)))\n",
    "\n",
    "    # Save the doc matrix as a pickle file\n",
    "    with open(f'{DATA_DIR}/doc_matrix.pkl', 'wb') as f:\n",
    "        pickle.dump(doc_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1471406x1130369 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 40408661 stored elements in List of Lists format>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_matrix = doc_matrix.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>query_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>300674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>125705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>94798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>174249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7432</th>\n",
       "      <td>7432</td>\n",
       "      <td>147073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7433</th>\n",
       "      <td>7433</td>\n",
       "      <td>243761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7434</th>\n",
       "      <td>7434</td>\n",
       "      <td>162662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7435</th>\n",
       "      <td>7435</td>\n",
       "      <td>247194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7436</th>\n",
       "      <td>7436</td>\n",
       "      <td>195199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7437 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  query_id\n",
       "0        0    300674\n",
       "1        1    125705\n",
       "2        2     94798\n",
       "3        3      9083\n",
       "4        4    174249\n",
       "...    ...       ...\n",
       "7432  7432    147073\n",
       "7433  7433    243761\n",
       "7434  7434    162662\n",
       "7435  7435    247194\n",
       "7436  7436    195199\n",
       "\n",
       "[7437 rows x 2 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db023edb7f904471aee84080e8e50da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing query matrix: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now create the query matrix where each row is a query\n",
    "# and each column is a word\n",
    "# the value of the cell is 1 if the word is in the query, 0 otherwise\n",
    "query_matrix = lil_matrix((len(test_data), len(vocabulary)))\n",
    "# iterate row by row\n",
    "for index, row in tqdm(test_data.iterrows(), desc=\"Computing query matrix\"):\n",
    "    # get the query from query_id\n",
    "    query_id = row['query_id']\n",
    "    query = queries_df.loc[query_id]\n",
    "    query = helpers.tokenize(query['text'], stemmer)\n",
    "    for term in query:\n",
    "        if term in vocabulary_dict:\n",
    "            term_id = vocabulary_dict[term]\n",
    "            query_matrix[index, term_id] = 1\n",
    "\n",
    "query_matrix = query_matrix.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can compute the scores for all the queries at once\n",
    "scores = query_matrix.dot(doc_matrix.T)\n",
    "# This gives us a matrix of shape (num_queries, num_documents)\n",
    "# Each cell contains the score of a document for a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Find the indices of the top 3 elements along each row\n",
    "k = 10\n",
    "top_k_indices = np.zeros((scores.shape[0], k), dtype=int)\n",
    "for i in range(scores.shape[0]):\n",
    "    row = scores.getrow(i).toarray()[0]\n",
    "    top_k_indices[i] = np.argpartition(row, -k)[-k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the document ids of the top k documents for each query\n",
    "top_k_doc_ids = []\n",
    "top_k_scores = []\n",
    "for index, row in enumerate(top_k_indices):\n",
    "    top_k_doc_ids.append([corpus_df.index.values[i] for i in row])\n",
    "    top_k_scores.append([scores[index, i] for i in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30.41874710329342,\n",
       " 31.99697828271954,\n",
       " 31.726511480146954,\n",
       " 36.30561337354201,\n",
       " 50.13671280047856,\n",
       " 52.375477845441964,\n",
       " 36.905421177995265,\n",
       " 43.60453799353136,\n",
       " 50.11667425529357,\n",
       " 46.69823514899414]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QueryID: 300674:\n",
      "Query: \"how many years did william bradford serve as governor of plymouth colony?\"\n",
      "Top 10 documents:\n",
      "\t Document ID: 4501973\n",
      "\t Text: \"(The Mayflower in Plymouth harbour by William Halsall, 1882). On this day in 1620, William Bradford and the Mayflower Pilgirms landed at Plymouth Rock in Plymouth, Massachusetts. The Mayflower transported the first English Pilgrims to America, with 102 passengers.\"\n",
      "\t Score: 30.41874710329342\n",
      "\n",
      "\t Document ID: 3872942\n",
      "\t Text: \"Plymouth Colony was founded by a group of people English separatists who later came to be known as the Pilgrims. The core group (roughly 40% of the adults and 56% of the family groupings) was part of a Congregationalist congregation led by William Bradford.\"\n",
      "\t Score: 31.99697828271954\n",
      "\n",
      "\t Document ID: 3305011\n",
      "\t Text: \"In addition, they settled as families for the most part, unique in Atlantic coast settlement at this point. Here we read from the journal of the colony's longtime governor, William Bradford, of the colonists' hard first year after landing in November 1620 to the first harvest in autumn 1621.\"\n",
      "\t Score: 31.726511480146954\n",
      "\n",
      "\t Document ID: 4190260\n",
      "\t Text: \"He was a signatory to the Mayflower Compact while aboard the Mayflower in 1620. He served as Plymouth Colony Governor five times covering about thirty years between 1621 and 1657. His journal, Of Plymouth Plantation, covered the period from 1620 to 1657 in Plymouth Colony.e was a signatory to the Mayflower Compact while aboard the Mayflower in 1620. He served as Plymouth Colony Governor five times covering about thirty years between 1621 and 1657. His journal, Of Plymouth Plantation, covered the period from 1620 to 1657 in Plymouth Colony.\"\n",
      "\t Score: 36.30561337354201\n",
      "\n",
      "\t Document ID: 7067032\n",
      "\t Text: \"http://en.wikipedia.org/wiki/William_Bradford_(Plymouth_Colony_governor) William Bradford (c.1590 â 1657) was an English Separatist leader in Leiden, Holland and in Plymouth Colony was a signatory to the Mayflower Compact. He served as Plymouth Colony Governor five times covering about thirty years between 1621 and 1657.\"\n",
      "\t Score: 50.13671280047856\n",
      "\n",
      "\t Document ID: 2495755\n",
      "\t Text: \"William Bradford (Mayflower passenger) William Bradford (1590 â 1657) was a passenger on the Mayflower in 1620. He travelled to the New World to live in religious freedom. He became the second Governor of Plymouth Colony and served for over 30 years. Bradford kept a journal of the history of the early life in Plymouth Colony. It is called Of Plymouth Plantation.\"\n",
      "\t Score: 52.375477845441964\n",
      "\n",
      "\t Document ID: 4665897\n",
      "\t Text: \"Pilgrims is a name commonly applied to early settlers of the Plymouth Colony in present-day Plymouth, Massachusetts, United States, with the men commonly called Pilgrim Fathers.illiam Bradford became governor in 1621 upon the death of John Carver. On March 22, 1621, the Pilgrims of Plymouth Colony signed a peace treaty with Massasoit of the Wampanoags. The patent of Plymouth Colony was surrendered by Bradford to the freemen in 1640, minus a small reserve of three tracts of land.\"\n",
      "\t Score: 36.905421177995265\n",
      "\n",
      "\t Document ID: 1266300\n",
      "\t Text: \"His father Noah Sr. (1722â1813) was a descendant of Connecticut Governor John Webster; his mother Mercy (Steele) Webster (1727â1794) was a descendant of Governor William Bradford of Plymouth Colony.\"\n",
      "\t Score: 43.60453799353136\n",
      "\n",
      "\t Document ID: 7067034\n",
      "\t Text: \"Home > Timelines > William Bradford Timeline William Bradford Timeline Timeline Description: William Bradford was a Separatist and a Pilgrim who traveled from England to America on the Mayflower. He is credited with being the author of the Mayflower Compact and served as the governor of Plymouth Colony.\"\n",
      "\t Score: 50.11667425529357\n",
      "\n",
      "\t Document ID: 4107182\n",
      "\t Text: \"The man to step forward in Plymouth colony was William Bradford. After the first governor elected under the Mayflower Compact perished from the harsh winter, Bradford was elected governor for the next thirty years. In May of 1621, he performed the colony's first marriage ceremony.\"\n",
      "\t Score: 46.69823514899414\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, row in test_data.iterrows():\n",
    "    print(f'QueryID: {row[\"query_id\"]}:')\n",
    "    print(f'Query: \"{queries_df.loc[row[\"query_id\"]][\"text\"]}\"')\n",
    "    print(f'Top {k} documents:')\n",
    "    for doc_id in top_k_doc_ids[index]:\n",
    "        print(f'\\t Document ID: {doc_id}')\n",
    "        print(f'\\t Text: \"{corpus_df.loc[doc_id][\"text\"]}\"')\n",
    "        # find row index of doc_id\n",
    "        row_index = corpus_df.index.get_loc(doc_id)\n",
    "        print(f'\\t Score: {scores[index, row_index]}')\n",
    "        print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_2 = pd.read_csv(f'{DATA_DIR}/task2_train.tsv', delimiter='\\t')\n",
    "test_data_2 = pd.read_csv(f'{DATA_DIR}/task2_test.tsv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the corpus-id and score columns contains string values,\n",
    "#this function converts them to a list of integers\n",
    "def extract_numbers(ids_str):\n",
    "    splitted = ids_str.split(\", \")\n",
    "    #Remove the square brackets in the first and last element\n",
    "    splitted[0] = splitted[0][1:]\n",
    "    splitted[-1] = splitted[-1][:-1]\n",
    "    return [int(id) for id in splitted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query-id</th>\n",
       "      <th>corpus-id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>915593</td>\n",
       "      <td>[1396701, 1396704, 1396705, 1396707, 1396708, ...</td>\n",
       "      <td>[0, 0, 1, 0, 2, 0, 3, 0, 0, 0, 2, 1, 2, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>146187</td>\n",
       "      <td>[1028971, 1028972, 1131101, 1138801, 1230566, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1114646</td>\n",
       "      <td>[1002453, 1216492, 1316103, 1316109, 1342262, ...</td>\n",
       "      <td>[0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1129237</td>\n",
       "      <td>[1020793, 1128332, 1138726, 1169301, 120308, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 3, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>573724</td>\n",
       "      <td>[1005338, 104856, 1053303, 1165128, 1165129, 1...</td>\n",
       "      <td>[1, 1, 0, 0, 1, 0, 0, 2, 2, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>148538</td>\n",
       "      <td>[1299824, 1299830, 1311202, 1311204, 1311206, ...</td>\n",
       "      <td>[2, 1, 2, 1, 0, 1, 1, 2, 1, 2, 2, 2, 0, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>527433</td>\n",
       "      <td>[1000485, 1101462, 1187918, 1212778, 1212782, ...</td>\n",
       "      <td>[3, 0, 0, 2, 2, 2, 0, 2, 0, 0, 0, 0, 0, 0, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>130510</td>\n",
       "      <td>[1046258, 1110766, 1156210, 1159414, 1211365, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 2, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>405717</td>\n",
       "      <td>[1111371, 1111372, 1111375, 1538943, 1538949, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1106007</td>\n",
       "      <td>[1020463, 1040867, 1195441, 1334328, 1334330, ...</td>\n",
       "      <td>[1, 0, 0, 2, 3, 2, 0, 2, 3, 3, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query-id                                          corpus-id  \\\n",
       "0    915593  [1396701, 1396704, 1396705, 1396707, 1396708, ...   \n",
       "1    146187  [1028971, 1028972, 1131101, 1138801, 1230566, ...   \n",
       "2   1114646  [1002453, 1216492, 1316103, 1316109, 1342262, ...   \n",
       "3   1129237  [1020793, 1128332, 1138726, 1169301, 120308, 1...   \n",
       "4    573724  [1005338, 104856, 1053303, 1165128, 1165129, 1...   \n",
       "5    148538  [1299824, 1299830, 1311202, 1311204, 1311206, ...   \n",
       "6    527433  [1000485, 1101462, 1187918, 1212778, 1212782, ...   \n",
       "7    130510  [1046258, 1110766, 1156210, 1159414, 1211365, ...   \n",
       "8    405717  [1111371, 1111372, 1111375, 1538943, 1538949, ...   \n",
       "9   1106007  [1020463, 1040867, 1195441, 1334328, 1334330, ...   \n",
       "\n",
       "                                               score  \n",
       "0  [0, 0, 1, 0, 2, 0, 3, 0, 0, 0, 2, 1, 2, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 3, 0, ...  \n",
       "4  [1, 1, 0, 0, 1, 0, 0, 2, 2, 0, 0, 0, 1, 0, 0, ...  \n",
       "5  [2, 1, 2, 1, 0, 1, 1, 2, 1, 2, 2, 2, 0, 1, 1, ...  \n",
       "6  [3, 0, 0, 2, 2, 2, 0, 2, 0, 0, 0, 0, 0, 0, 3, ...  \n",
       "7  [0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 2, 2, ...  \n",
       "8  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n",
       "9  [1, 0, 0, 2, 3, 2, 0, 2, 3, 3, 1, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1fb1c435b44eb6abd24d4697198f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing query matrixvfor task 2: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now create the query matrix where each row is a query\n",
    "# and each column is a word\n",
    "# the value of the cell is 1 if the word is in the query, 0 otherwise\n",
    "query_matrix_2 = lil_matrix((len(test_data_2), len(vocabulary)))\n",
    "# iterate row by row\n",
    "for index, row in tqdm(test_data_2.iterrows(), desc=\"Computing query matrixvfor task 2\"):\n",
    "    # get the query from query_id\n",
    "    query_id = row['query-id']\n",
    "    query = queries_df.loc[query_id]\n",
    "    query = helpers.tokenize(query['text'], stemmer)\n",
    "    for term in query:\n",
    "        if term in vocabulary_dict:\n",
    "            term_id = vocabulary_dict[term]\n",
    "            query_matrix_2[index, term_id] = 1\n",
    "\n",
    "query_matrix_2 = query_matrix_2.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_task_2 = query_matrix_2.dot(doc_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1471406 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2377 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_task_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QueryID: 1115776:\n",
      "Query: \"what is an aml surveillance analyst\"\n",
      "Top 10 documents:\n",
      "1462410\n",
      "\t Document ID: 8106324\n",
      "\t Text: \"Be the first to see new Aml Surveillance Analyst jobs My email: Also get an email with jobs recommended just for me Anti Money Laundering Analyst salaries in United States\"\n",
      "\t Score: 30.13295554433937\n",
      "\t Document ID: 8106323\n",
      "\t Text: \"NEXT Financial Surveillance Analyst: 1 NEXT Financial Surveillance Analyst salaries $54,259 Philadelphia Stock Exchange Surveillance Analyst: 1 Philadelphia Stock Exchange Surveillance Analyst salaries\"\n",
      "\t Score: 25.13075050876806\n",
      "\t Document ID: 4314171\n",
      "\t Text: \"Today's top 9449 Aml Analyst jobs in United States. Leverage your professional network, and get hired. New Aml Analyst jobs added daily.\"\n",
      "\t Score: 23.928872683499506\n",
      "\t Document ID: 600966\n",
      "\t Text: \"average casino surveillance analyst salaries for job postings in reno nv are 19 % lower than average casino surveillance analyst salaries for job postings nationwideverage casino surveillance analyst salaries for job postings in reno nv are 19 % lower than average casino surveillance analyst salaries for job postings nationwide\"\n",
      "\t Score: 23.67121371170827\n",
      "\t Document ID: 8106318\n",
      "\t Text: \"769 Aml Analyst jobs. Find your next opportunity on Simply Hired. New jobs are posted every day. Simply Hired makes your Aml Analyst job search easier by showing the newest jobs with salary estimates, company ratings, and more. There are over 769 Aml Analyst positions available Search Jobs\"\n",
      "\t Score: 23.619187467707967\n",
      "\t Document ID: 8106321\n",
      "\t Text: \"AML Analysts are responsible for detecting and monitoring suspicious transactions in an effort to prevent money laundering. A typical AML Analyst resume usually describes duties such as implementing anti money laundering procedures, assessing daily financial activity, and preparing regular reports to AML Compliance Officers.\"\n",
      "\t Score: 22.89589062138828\n",
      "\t Document ID: 5605159\n",
      "\t Text: \"Hence, NK cells are considered to be important in the immune surveillance of cancer. In acute myeloid leukemia (AML) patients, however, significantly impaired NK cell functions can facilitate escape from immune surveillance and affect patient outcome.\"\n",
      "\t Score: 21.67799900592226\n",
      "\t Document ID: 4314178\n",
      "\t Text: \"If you have two (2) plus yearsâ of experience as an AML Analyst then this may be the next perfect position for you. Two plus years of AML Compliance or fraud related experience. www.specialcounsel.com\"\n",
      "\t Score: 21.527459404712793\n",
      "\t Document ID: 4314176\n",
      "\t Text: \"The BSA / AML Analyst is responsible for monitoring and investigating customer transactions under applicable anti-money laundering and anti-terrorist financing laws.\"\n",
      "\t Score: 20.260060463488795\n",
      "\t Document ID: 5605160\n",
      "\t Text: \"Increased knowledge on AML escape routes from NK cell immune surveillance will further aid in the design of novel NK cell-based immunotherapy approaches for the treatment of AML.Leukemia accepted article preview online, 26 March 2012; doi:10.1038/leu.2012.87.\"\n",
      "\t Score: 19.299390295685512\n"
     ]
    }
   ],
   "source": [
    "for index, row in test_data_2.iterrows():\n",
    "    print(f'QueryID: {row[\"query-id\"]}:')\n",
    "    print(f'Query: \"{queries_df.loc[row[\"query-id\"]][\"text\"]}\"')\n",
    "    print(f'Top {k} documents:')\n",
    "\n",
    "    corpus_ids = extract_numbers(row[\"corpus-id\"])\n",
    "    print(row_index)\n",
    "    sorted_scores_task_2 = []\n",
    "    for doc_id in corpus_ids:\n",
    "        #print(f'\\t Document ID: {doc_id}')\n",
    "        #print(f'\\t Text: \"{corpus_df.loc[doc_id][\"text\"]}\"')\n",
    "        # find row index of doc_id\n",
    "        row_index = corpus_df.index.get_loc(doc_id)\n",
    "        sorted_scores_task_2.append((doc_id, scores_task_2[index, row_index]))\n",
    "        #print(f'\\t Score: {scores_task_2[index, row_index]}')\n",
    "        #print()\n",
    "\n",
    "    sorted_scores_task_2 = sorted(sorted_scores_task_2, key = lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "    for entry in sorted_scores_task_2:\n",
    "        doc_id, score = entry[0], entry[1]\n",
    "        print(f'\\t Document ID: {doc_id}')\n",
    "        print(f'\\t Text: \"{corpus_df.loc[doc_id][\"text\"]}\"')\n",
    "        print(f'\\t Score: {score}')\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scores_task_22 = []\\n\\nfor row in test_data_2.iloc:\\n    corpus_ids = extract_numbers(row[\"corpus-id\"])\\n    query_id = row[\"query-id\"]\\n    query_index = queries_df.index.get_loc(query_id)\\n\\n    query = queries_df[\"text\"].iloc[query_index]\\n    query = helpers.tokenize(query,stemmer)\\n    scores_inter = []\\n    \\n    for i, corpus_id in enumerate(corpus_ids):\\n        corpus_index = corpus_df.index.get_loc(corpus_id)\\n        score = helpers.bm25(documents[corpus_index], query, idfs, avg_doc_length, k1 = 1, b = 0.75)\\n        scores_inter.append((corpus_id, score))\\n    sorted_scores = sorted(scores_inter, key=lambda x: x[1], reverse=True)[:10]\\n    scores_task_22.append(sorted_scores)    \\n\\nscores_task_22[0]'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Old code that computes the scores using nested for loop\n",
    "\"\"\"scores_task_22 = []\n",
    "\n",
    "for row in test_data_2.iloc:\n",
    "    corpus_ids = extract_numbers(row[\"corpus-id\"])\n",
    "    query_id = row[\"query-id\"]\n",
    "    query_index = queries_df.index.get_loc(query_id)\n",
    "\n",
    "    query = queries_df[\"text\"].iloc[query_index]\n",
    "    query = helpers.tokenize(query,stemmer)\n",
    "    scores_inter = []\n",
    "    \n",
    "    for i, corpus_id in enumerate(corpus_ids):\n",
    "        corpus_index = corpus_df.index.get_loc(corpus_id)\n",
    "        score = helpers.bm25(documents[corpus_index], query, idfs, avg_doc_length, k1 = 1, b = 0.75)\n",
    "        scores_inter.append((corpus_id, score))\n",
    "    sorted_scores = sorted(scores_inter, key=lambda x: x[1], reverse=True)[:10]\n",
    "    scores_task_22.append(sorted_scores)    \n",
    "\n",
    "scores_task_22[0]\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
