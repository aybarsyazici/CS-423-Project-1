{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import helpers\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import multiprocessing as mp\n",
    "import os \n",
    "import math\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "# Load the data from files\n",
    "with open(f'{DATA_DIR}/corpus.jsonl', 'r') as f:\n",
    "    corpus_data = {int(item['_id']): item['text'] for item in (json.loads(line) for line in f)}\n",
    "\n",
    "with open(f'{DATA_DIR}/queries.jsonl', 'r') as f:\n",
    "    queries_data = {int(item['_id']): item['text'] for item in (json.loads(line) for line in f)}\n",
    "\n",
    "train_data = pd.read_csv(f'{DATA_DIR}/task1_train.tsv', delimiter='\\t')\n",
    "test_data = pd.read_csv(f'{DATA_DIR}/task1_test.tsv', delimiter='\\t')\n",
    "\n",
    "# Rename corpus-id to document_id and query-id to query_id in both train and test data\n",
    "train_data = train_data.rename(columns={'corpus-id': 'document_id', 'query-id': 'query_id'})\n",
    "test_data = test_data.rename(columns={'corpus-id': 'document_id', 'query-id': 'query_id'})\n",
    "# Make sure that the document_id and query_id are int64\n",
    "train_data['document_id'] = train_data['document_id'].astype('int64')\n",
    "train_data['query_id'] = train_data['query_id'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a df from the corpus data\n",
    "corpus_df = pd.DataFrame.from_dict(corpus_data, orient='index', columns=['text'])\n",
    "# Create a df from the queries data\n",
    "queries_df = pd.DataFrame.from_dict(queries_data, orient='index', columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenized documents from pickle file...\n"
     ]
    }
   ],
   "source": [
    "# import partial\n",
    "from functools import partial\n",
    "\n",
    "# Check if documents.pkl exists:\n",
    "if os.path.isfile(f'{DATA_DIR}/documents.pkl'):\n",
    "    print('Loading tokenized documents from pickle file...')\n",
    "    # load the tokenized documents from pickle file\n",
    "    import pickle\n",
    "    with open(f'{DATA_DIR}/documents.pkl', 'rb') as f:\n",
    "        documents = pickle.load(f)\n",
    "else:\n",
    "    print('File not found. Tokenizing documents...')\n",
    "    documents = corpus_df['text'].tolist()\n",
    "    documents = [x.strip() for x in documents]\n",
    "    # use multiprocessing to speed up the process\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    # pass both documents and stemmer as arguments to the tokenize function\n",
    "    fn = partial(helpers.tokenize, stemmer=stemmer)   \n",
    "    documents = list(tqdm(pool.imap(fn, documents), total=len(documents))) \n",
    "    # save the tokenized documents as pickle file\n",
    "    import pickle\n",
    "    with open(f'{DATA_DIR}/documents.pkl', 'wb') as f:\n",
    "        pickle.dump(documents, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = list(set([item for sublist in documents for item in sublist]))\n",
    "vocabulary.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8afce1642424812bc27dc967115eb9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1471406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute the number of documents that contain each word\n",
    "doc_freqs = {}\n",
    "for doc in tqdm(documents):\n",
    "    for word in set(doc):\n",
    "        doc_freqs[word] = doc_freqs.get(word, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the IDF values for each word in the vocabulary\n",
    "# num_docs = len(documents)\n",
    "# idf = {}\n",
    "# for word in tqdm(vocabulary):\n",
    "#     doc_freq = doc_freqs.get(word, 0)\n",
    "#     idf[word] = math.log(num_docs / (doc_freq + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the IDF for each word in the vocabulary\n",
    "num_docs = len(documents)\n",
    "idfs = {word: math.log(num_docs / freq) for word, freq in doc_freqs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8b5c6dab904db3ae10cfc2b213ea41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a vocabulary dictionary with the index of each word in the vocabulary\n",
    "vocabulary_dict = {word: i for i, word in tqdm(enumerate(vocabulary))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413b408156e3439f9a07a54cc0740600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing term frequency matrix:   0%|          | 0/1471406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "# Compute the term frequency matrix\n",
    "term_freq_matrix = lil_matrix((len(documents), len(vocabulary))) # We use lil_matrix since it is efficient in incremental assignments\n",
    "for i, doc in tqdm(enumerate(documents), total=len(documents), desc='Computing term frequency matrix'):\n",
    "    counts, max_count = helpers.count_terms(doc)\n",
    "    for term, count in counts.items():\n",
    "        if term in vocabulary_dict:\n",
    "            term_id = vocabulary_dict[term]\n",
    "            term_freq_matrix[i, term_id] = count/max_count * idfs[term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1471406x1130369 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 40408661 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since the lil_matrix is inefficient for row slicing/matrix multiplication, convert it to csr_matrix\n",
    "term_freq_matrix_csr = term_freq_matrix.tocsr()\n",
    "term_freq_matrix_csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get the first row of the query matrix\n",
    "query = queries_df['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the query\n",
    "query = helpers.tokenize(query, stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the tf-idf of the query\n",
    "query_vec = helpers.compute_query_vector(query, vocabulary_dict, idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the cosine similarity between the query and the documents\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarities = cosine_similarity(term_freq_matrix_csr, query_vec.reshape(1,-1))\n",
    "\n",
    "# get the top 10 documents\n",
    "top_docs = np.argsort(cosine_similarities, axis=0)[::-1][:10]\n",
    "top_docs = top_docs.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1471406"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 3607205\n",
      "Text: \"Manhattan Project. 1  The Manhattan Project was a secret military project created in 1942 to produce the first US nuclear weapon. Fears that Nazi Germany would build and use a nuclear weapon during World War II triggered the start of the Manhattan Project, which was originally based in Manhattan, New York.\"\n",
      "Similarity: [0.52951184]\n",
      "\n",
      "Document ID: 7243450\n",
      "Text: \"The project was given its name due to the fact that at least 10 of the sites used for the research were located in Manhattan. Following is a timeline of the key events related to the development of the atomic bomb and the Manhattan Project. Manhattan Project Timeline\"\n",
      "Similarity: [0.51425655]\n",
      "\n",
      "Document ID: 2036644\n",
      "Text: \"Manhattan Project. The Manhattan Project was a secret military project created in 1942 to produce the first US nuclear weapon. Fears that Nazi Germany would build and use a nuclear weapon during World War II triggered the start of the Manhattan Project, which was originally based in Manhattan, New York.anhattan Project. The Manhattan Project was a secret military project created in 1942 to produce the first US nuclear weapon. Fears that Nazi Germany would build and use a nuclear weapon during World War II triggered the start of the Manhattan Project, which was originally based in Manhattan, New York.\"\n",
      "Similarity: [0.50376111]\n",
      "\n",
      "Document ID: 3870080\n",
      "Text: \"Manhattan Project. The Manhattan Project was a secret military project created in 1942 to produce the first US nuclear weapon. Fears that Nazi Germany would build and use a nuclear weapon during World War II triggered the start of the Manhattan Project, which was originally based in Manhattan, New York.he Manhattan Project was a secret military project created in 1942 to produce the first US nuclear weapon. Fears that Nazi Germany would build and use a nuclear weapon during World War II triggered the start of the Manhattan Project, which was originally based in Manhattan, New York.\"\n",
      "Similarity: [0.49692316]\n",
      "\n",
      "Document ID: 3870082\n",
      "Text: \"The Manhattan Project was a research and development project that produced the first nuclear weapons during World War II.he Army component of the project was designated the Manhattan District; Manhattan gradually superseded the official codename, Development of Substitute Materials, for the entire project. Along the way, the project absorbed its earlier British counterpart, Tube Alloys.\"\n",
      "Similarity: [0.47641881]\n",
      "\n",
      "Document ID: 2395246\n",
      "Text: \"The Manhattan Project was a research and development project that produced the first atomic bombs during World War II. Here's an answer: The Manhattan Project was a movie made about Peter Stuyvesant's quest to buy the island of Manhattan from Native Americans back in the 1600s. For a better answer, look below.\"\n",
      "Similarity: [0.47419566]\n",
      "\n",
      "Document ID: 5117689\n",
      "Text: \"The Manhattan Project -- Its Operations. Major operations for the Manhattan Engineer District (Manhattan Project) took place in remote site locations in the states of Tennessee, New Mexico, and Washington, with additional research being conducted in university laboratories at Chicago and Berkeley.\"\n",
      "Similarity: [0.47416411]\n",
      "\n",
      "Document ID: 4138462\n",
      "Text: \"The Manhattan Project was a secret military project created in 1942 to produce the first US nuclear weapon. Fears that Nazi Germany would build and use a nuclear weapon during World War II triggered the start of the Manhattan Project, which was originally based in Manhattan, New York.\"\n",
      "Similarity: [0.46096246]\n",
      "\n",
      "Document ID: 6108324\n",
      "Text: \"The Manhattan Project. The Manhattan Project was a research and development program, led by the United States with participation from the United Kingdom and Canada, that produced the first atomic bomb during World War II. It was also charged with gathering intelligence on the German nuclear energy project.\"\n",
      "Similarity: [0.43342178]\n",
      "\n",
      "Document ID: 2148554\n",
      "Text: \"This article is about the atomic bomb project. For other uses, see Manhattan Project (disambiguation). The Manhattan Project was a research and development undertaking during World War II that produced the first nuclear weapons. It was led by the United States with the support of the United Kingdom and Canada.\"\n",
      "Similarity: [0.42934064]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index in top_docs:\n",
    "    print(f'Document ID: {corpus_df.index.values[index]}')\n",
    "    print(f'Text: \"{corpus_df.iloc[index].text}\"')\n",
    "    print(f'Similarity: {cosine_similarities[index]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561989f71a044d83941ce62981df691e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test data:   0%|          | 0/7437 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# do it for each query in test data\n",
    "# create a list to store the results\n",
    "results = []\n",
    "\n",
    "# iterate over each query in the test data\n",
    "for row in tqdm(test_data.iterrows(), total=len(test_data), desc='Processing test data'):\n",
    "    query_id = row[1][\"query_id\"]\n",
    "    query = queries_df.loc[query_id]['text']\n",
    "    query = helpers.tokenize(query, stemmer)\n",
    "    query_vec = helpers.compute_query_vector(query, vocabulary_dict, idfs)\n",
    "    cosine_similarities = cosine_similarity(term_freq_matrix_csr, query_vec.reshape(1,-1))\n",
    "    top_docs = np.argsort(cosine_similarities, axis=0)[::-1][:10]\n",
    "    top_docs = top_docs.flatten()\n",
    "\n",
    "    # get document_ids\n",
    "    document_ids = corpus_df.index.values[top_docs]\n",
    "    results.append(document_ids)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9d2c4f854c46fe8b3b3d0dc9c006bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing query matrix:   0%|          | 0/7437 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a matrix for queries\n",
    "query_matrix = lil_matrix((len(test_data), len(vocabulary)))\n",
    "\n",
    "# iterate over each query in the test data\n",
    "for index, row in tqdm(test_data.iterrows(), desc=\"Computing query matrix\", total=len(test_data)):\n",
    "    # get the query from query_id\n",
    "    query_id = row['query_id']\n",
    "    query = queries_df.loc[query_id]\n",
    "    query = helpers.tokenize(query['text'], stemmer)\n",
    "    for term in query:\n",
    "        if term in vocabulary_dict:\n",
    "            term_id = vocabulary_dict[term]\n",
    "            query_vec[term_id] = count/max_count * idfs[term]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the cosine similarity between the query and the documents\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarities = cosine_similarity(query_matrix.tocsr(), term_freq_matrix_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each query, get the top 10 documents\n",
    "# top_docs = np.argsort(cosine_similarities, axis=1)[:,::-1][:,:10]\n",
    "# Doing this way uses too much memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
