{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import helpers\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import multiprocessing as mp\n",
    "import os \n",
    "import math\n",
    "from functools import partial\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "# Load the data from files\n",
    "with open(f'{DATA_DIR}/corpus.jsonl', 'r') as f:\n",
    "    corpus_data = {int(item['_id']): item['text'] for item in (json.loads(line) for line in f)}\n",
    "\n",
    "with open(f'{DATA_DIR}/queries.jsonl', 'r') as f:\n",
    "    queries_data = {int(item['_id']): item['text'] for item in (json.loads(line) for line in f)}\n",
    "\n",
    "train_data = pd.read_csv(f'{DATA_DIR}/task1_train.tsv', delimiter='\\t')\n",
    "test_data = pd.read_csv(f'{DATA_DIR}/task1_test.tsv', delimiter='\\t')\n",
    "\n",
    "# Rename corpus-id to document_id and query-id to query_id in both train and test data\n",
    "train_data = train_data.rename(columns={'corpus-id': 'document_id', 'query-id': 'query_id'})\n",
    "test_data = test_data.rename(columns={'corpus-id': 'document_id', 'query-id': 'query_id'})\n",
    "# Make sure that the document_id and query_id are int64\n",
    "train_data['document_id'] = train_data['document_id'].astype('int64')\n",
    "train_data['query_id'] = train_data['query_id'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a df from the corpus data\n",
    "corpus_df = pd.DataFrame.from_dict(corpus_data, orient='index', columns=['text'])\n",
    "# Create a df from the queries data\n",
    "queries_df = pd.DataFrame.from_dict(queries_data, orient='index', columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_documents = corpus_df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a file for preprocessed documents exists\n",
    "if os.path.isfile(f'{DATA_DIR}/preprocessed_documents.txt'):\n",
    "    # if it exists, read the preprocessed documents from the file\n",
    "    with open(f'{DATA_DIR}/preprocessed_documents.txt', 'r') as f:\n",
    "        preprocessed_documents = f.readlines()\n",
    "else:\n",
    "    documents = corpus_df['text'].tolist()\n",
    "    documents = [x.strip() for x in documents]\n",
    "    # use multiprocessing to speed up the process\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    # pass both documents and stemmer as arguments to the tokenize function\n",
    "    fn = partial(helpers.tokenize, stemmer=stemmer)   \n",
    "    preprocessed_documents = list(tqdm(pool.imap(fn, documents), total=len(documents))) \n",
    "\n",
    "    # write preprocessed documents to a txt file\n",
    "    with open(f'{DATA_DIR}/preprocessed_documents.txt', 'w') as f:\n",
    "        for item in preprocessed_documents:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a file for preprocessed queries already exists\n",
    "if os.path.exists(f'{DATA_DIR}/preprocessed_queries.txt'):\n",
    "    # If it exists, load the preprocessed queries from the file\n",
    "    with open(f'{DATA_DIR}/preprocessed_queries.txt', 'r') as f:\n",
    "        preprocessed_queries = f.readlines()\n",
    "else:\n",
    "    # preprocess queries in the same way as documents\n",
    "    queries = queries_df['text'].tolist()\n",
    "    queries = [x.strip() for x in queries]\n",
    "    # use multiprocessing to speed up the process\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    # pass both queries and stemmer as arguments to the tokenize function\n",
    "    fn = partial(helpers.tokenize, stemmer=stemmer)\n",
    "    preprocessed_queries = list(tqdm(pool.imap(fn, queries), total=len(queries)))\n",
    "\n",
    "    # write preprocessed queries to a txt file\n",
    "    with open(f'{DATA_DIR}/preprocessed_queries.txt', 'w') as f:\n",
    "        for item in preprocessed_queries:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 54M words\n",
      "Number of words:  150831\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   75585 lr:  0.000000 avg.loss:  0.759930 ETA:   0h 0m 0s 45.2% words/sec/thread:   74589 lr:  0.027423 avg.loss:  1.287198 ETA:   0h 3m42s\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "model = fasttext.train_unsupervised(f'{DATA_DIR}/preprocessed_documents.txt', model = 'skipgram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save_model(f'{DATA_DIR}/fasttext_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = model.words\n",
    "word_embeddings = np.array([model[word] for word in vocabulary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1f4e7dbe0849acb3e861bace5937b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b5477a2e97426da17c38f2d6a4c382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1471406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e82ece45cf4132a0f7d035eac2077c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1471406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5281bbd43c9480e97e02b0ac112a4e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1471406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a dictionary of vectors for easier search\n",
    "vector_dict = dict(zip(vocabulary, word_embeddings))\n",
    "\n",
    "def aggregate_vector_list(vlist, aggfunc):\n",
    "    if aggfunc == 'max':\n",
    "        return np.array(vlist).max(axis=0)\n",
    "    elif aggfunc == 'min':\n",
    "        return np.array(vlist).min(axis=0)\n",
    "    elif aggfunc == 'mean':\n",
    "        return np.array(vlist).mean(axis=0)\n",
    "    else:\n",
    "        return np.zeros(np.array(vlist).shape[1])\n",
    "\n",
    "possible_aggfuncs = [\"max\", \"min\", \"mean\"]\n",
    "\n",
    "aggregated_doc_vectors = {}\n",
    "\n",
    "# Aggregate vectors of documents beforehand\n",
    "for aggfunc in tqdm(possible_aggfuncs):\n",
    "    aggregated_doc_vectors[aggfunc] = np.zeros((len(preprocessed_documents), word_embeddings.shape[1]))\n",
    "    for index, doc in tqdm(enumerate(preprocessed_documents), total=len(preprocessed_documents)):\n",
    "        vlist = [vector_dict[token] for token in fasttext.tokenize(doc) if token in vector_dict]\n",
    "        if(len(vlist) < 1):\n",
    "            continue \n",
    "        else:\n",
    "            aggregated_doc_vectors[aggfunc][index] = aggregate_vector_list(vlist, aggfunc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1471406, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_doc_vectors['mean'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def aggregate_query(query, aggfunc):\n",
    "    tokens = fasttext.tokenize(query)\n",
    "    if(len(tokens) == 1):\n",
    "        if(tokens[0] in vocabulary):\n",
    "            return vector_dict[tokens[0]]\n",
    "    elif(len(tokens) > 1):\n",
    "        vlist = []\n",
    "        print('tokens are ', tokens)\n",
    "        for token in tokens:\n",
    "            if (token in vocabulary):\n",
    "                vlist.append(vector_dict[token])\n",
    "        return aggregate_vector_list(vlist, aggfunc)\n",
    "    else:\n",
    "        print(\"%s is not in the vocabulary.\" % (query))\n",
    "    \n",
    "def get_most_similar_documents(query_vector, aggfunc, k = 5):\n",
    "    query_vector = query_vector.reshape(1, -1)\n",
    "    # Calculate the similarity with each vector. \n",
    "    # Hint: Cosine similarity function takes a matrix as input so you do not need to loop through each document vector.\n",
    "    sim = cosine_similarity(query_vector, aggregated_doc_vectors[aggfunc])\n",
    "    print(sim.shape)\n",
    "    # Rank the document vectors according to their cosine similarity with \n",
    "    indexes = np.argsort(sim, axis=-1, kind='quicksort', order=None) # This is sorted in ascending order\n",
    "    indexes = indexes[0]\n",
    "    indexes = indexes[::-1] # Convert to descending\n",
    "    return indexes\n",
    "\n",
    "def search_vec_embeddings(query, topk = 10, aggfunc = 'mean'):\n",
    "    query_vector = aggregate_query(query, aggfunc)\n",
    "    print(\"Query vector shape: \", query_vector.shape)\n",
    "    indexes = get_most_similar_documents(query_vector, aggfunc)\n",
    "    # Print the top k documents\n",
    "    indexes = indexes[0:topk]\n",
    "    for index in indexes:\n",
    "        print(f'Document id: {corpus_df.iloc[index].name}')\n",
    "        print(original_documents[index])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "')what was the immediate impact of the success of the manhattan project?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the first query\n",
    "query = queries_df.iloc[0]['text']\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'immedi impact success manhattan project'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = helpers.tokenize(query, stemmer=stemmer)\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens are  ['immedi', 'impact', 'success', 'manhattan', 'project']\n",
      "Query vector shape:  (100,)\n",
      "(1, 1471406)\n",
      "Document id: 33625\n",
      "Maryann Johnson, a real-estate agent in Manhattan, starts her days with a motivational Instagram post. â\n",
      "\n",
      "Document id: 3027617\n",
      "Upper Manhattan is a large and fascinating place where the identity and characteristics of the neighborhoods change almost every few blocks. Harlem itself consists of several neighborhoods each with its own distinct culture and history.\n",
      "\n",
      "Document id: 7397017\n",
      "Over on Reddit, a user named movielover278 posted a picture he created that shows a size comparison between a Super Star Destroyer from the Star Wars franchise Over on Reddit, a user named movielover278 posted a picture he created that shows a size comparison between a Super Star Destroyer from the Star Wars franchise and Manhattan, New York.\n",
      "\n",
      "Document id: 5079447\n",
      "One of New York Cityâs most famous landmarks is the Wall Street Bull, located at Bowling Green Park in Lower Manhattan. However not many visitors, or even locals, know about the events that led to the bull being located where it is.\n",
      "\n",
      "Document id: 1447684\n",
      "Utica, and Albany area of New York. Meet Our Surgeons Meet the expertly trained Albany plastic surgeons at The Plastic Surgery Group, serving the Schenectady, Saratoga Springs, Utica, and Albany area of New York.\n",
      "\n",
      "Document id: 8321043\n",
      "Midtown is where you'll find most of New York's skyscrapers, along. with many of the things you'll want to see while visiting the city. Midtown: Most of Manhattan's skyscrapers are found in Midtown, along with most of its major hotels, theaters, department stores, and famous tourist attractions.\n",
      "\n",
      "Document id: 2549109\n",
      "The Avengers. Avengers Tower (previously known as Stark Tower) is a high-rise building complex in the 2012 Marvel action film The Avengers owned by Tony Stark. It is located in Manhattan, New York City. It serves as the headquarters of the superhero team, The Avengers.\n",
      "\n",
      "Document id: 1109477\n",
      "New York City, NY; NYC. This is a 3-hour tour that begins with a walk over the Brooklyn Bridge, an icon of New York City for over 125 years, with spectacular views of Manhattan and Brooklyn.\n",
      "\n",
      "Document id: 4050123\n",
      "Alexandra is an American citizen. Education. When she was a child, Daddario went to an all-girls school called Brearley School. By the time she was 16 years old, she chose to attend the Professional Childrenâs School in New York City. Afterward, she studied at Marymount Manhattan College.\n",
      "\n",
      "Document id: 7246140\n",
      "JCRP--jointly administered by EDC and ESD--is designed to stabilize the job base in Lower Manhattan, restore the vibrancy of the downtown business community, encourage the creation of new jobs, attract new businesses and diversify the downtown economy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_vec_embeddings(query, aggfunc = 'max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
