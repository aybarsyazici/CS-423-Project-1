{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aybarsyazici/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aybarsyazici/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import helpers\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import multiprocessing as mp\n",
    "import os \n",
    "import math\n",
    "from functools import partial\n",
    "from nltk import download as nltk_download\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords, strip_short, stem_text\n",
    "import pickle\n",
    "\n",
    "nltk_download('punkt')\n",
    "nltk_download('stopwords')\n",
    "\n",
    "DATA_DIR = 'data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "# Load the data from files\n",
    "with open(f'{DATA_DIR}/corpus.jsonl', 'r') as f:\n",
    "    corpus_data = {int(item['_id']): item['text'] for item in (json.loads(line) for line in f)}\n",
    "\n",
    "\n",
    "#train_data = pd.read_csv(f'{DATA_DIR}/task1_train.tsv', delimiter='\\t')\n",
    "\n",
    "# Rename corpus-id to document_id and query-id to query_id in both train and test data\n",
    "#train_data = train_data.rename(columns={'corpus-id': 'document_id', 'query-id': 'query_id'})\n",
    "# Make sure that the document_id and query_id are int64\n",
    "#train_data['document_id'] = train_data['document_id'].astype('int64')\n",
    "#train_data['query_id'] = train_data['query_id'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{DATA_DIR}/queries.jsonl', 'r') as f:\n",
    "    queries_data = {int(item['_id']): item['text'] for item in (json.loads(line) for line in f)}\n",
    "\n",
    "test_data = pd.read_csv(f'{DATA_DIR}/task1_test.tsv', delimiter='\\t')\n",
    "test_data = test_data.rename(columns={'corpus-id': 'document_id', 'query-id': 'query_id'})\n",
    "\n",
    "CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation, lambda x: strip_short(s=x,minsize=1), strip_multiple_whitespaces, stem_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>query_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>300674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>125705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>94798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>174249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7432</th>\n",
       "      <td>7432</td>\n",
       "      <td>147073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7433</th>\n",
       "      <td>7433</td>\n",
       "      <td>243761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7434</th>\n",
       "      <td>7434</td>\n",
       "      <td>162662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7435</th>\n",
       "      <td>7435</td>\n",
       "      <td>247194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7436</th>\n",
       "      <td>7436</td>\n",
       "      <td>195199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7437 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  query_id\n",
       "0        0    300674\n",
       "1        1    125705\n",
       "2        2     94798\n",
       "3        3      9083\n",
       "4        4    174249\n",
       "...    ...       ...\n",
       "7432  7432    147073\n",
       "7433  7433    243761\n",
       "7434  7434    162662\n",
       "7435  7435    247194\n",
       "7436  7436    195199\n",
       "\n",
       "[7437 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation, lambda x: strip_short(s=x,minsize=1), strip_multiple_whitespaces, stem_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a df from the corpus data\n",
    "corpus_df = pd.DataFrame.from_dict(corpus_data, orient='index', columns=['text'])\n",
    "# Create a df from the queries data\n",
    "queries_df = pd.DataFrame.from_dict(queries_data, orient='index', columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a file for preprocessed documents exists\n",
    "if os.path.isfile(f'{DATA_DIR}/preprocessed_documents_3.txt'):\n",
    "    # if it exists, read the preprocessed documents from the file\n",
    "    with open(f'{DATA_DIR}/preprocessed_documents_3.txt', 'r') as f:\n",
    "        preprocessed_documents = f.readlines()\n",
    "else:\n",
    "    documents = corpus_df.text.values.tolist()\n",
    "    print(documents[:5])\n",
    "    preprocessed_documents = [preprocess_string(document, CUSTOM_FILTERS) for document in tqdm(documents, desc='Preprocessing documents', total=len(documents))]\n",
    "    # use multiprocessing to speed up the process\n",
    "    # pool = mp.Pool(mp.cpu_count())\n",
    "    # pass both documents and stemmer as arguments to the tokenize function\n",
    "    # fn = partial(helpers.tokenize, stemmer=stemmer)   \n",
    "    # preprocessed_documents = list(tqdm(pool.imap(fn, documents), total=len(documents))) \n",
    "\n",
    "    # write preprocessed documents to a txt file\n",
    "    with open(f'{DATA_DIR}/preprocessed_documents_3.txt', 'w') as f:\n",
    "        # join the array of tokens to a string and write it to the file\n",
    "        f.writelines([' '.join(document) + '\\n' for document in preprocessed_documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = {\n",
    "    'trivora': ['ethinylestradiol', 'levonorgestrel'],\n",
    "    'rovna': ['rovn\\u00c3\\u00a1'.lower()],\n",
    "    'carlomagno': ['charlemagn']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "# epoch parameter is by default set to 5\n",
    "# print(f'Using {mp.cpu_count()} cores')\n",
    "# model = fasttext.train_unsupervised(f'{DATA_DIR}/preprocessed_documents_3.txt', model = 'skipgram', thread=mp.cpu_count(), verbose=2, ws=5, dim=450, epoch=10, minCount=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# model.save_model(f'{DATA_DIR}/fasttext_model_skipgram_ws5_3.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "model = fasttext.load_model(f'{DATA_DIR}/fasttext_model_skipgram_ws5_3.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = model.words\n",
    "word_embeddings = np.array([model[word] for word in vocabulary])\n",
    "# Create a dictionary of vectors for easier search\n",
    "vector_dict = dict(zip(vocabulary, word_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0015f169ff6443be9ea7f1182bc862b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1471406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Compute the number of documents that contain each word\n",
    "doc_freqs = {}\n",
    "for doc in tqdm(preprocessed_documents):\n",
    "    for word in set(fasttext.tokenize(doc.strip())):\n",
    "        doc_freqs[word] = doc_freqs.get(word, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = len(preprocessed_documents)\n",
    "# idfs = {word: math.log(num_docs / freq) for word, freq in doc_freqs.items()}\n",
    "idfs = {word: math.log((num_docs - freq + 0.5)/(freq + 0.5)+1) for word, freq in doc_freqs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter , defaultdict\n",
    "\n",
    "def aggregate_vector_list(vlist, aggfunc, weights=None, log = False):\n",
    "    if log: \n",
    "        print(weights)\n",
    "    if aggfunc == 'idf':\n",
    "        return np.average(vlist, axis=0, weights=weights)\n",
    "    elif aggfunc == 'mean':\n",
    "        return np.array(vlist).mean(axis=0)\n",
    "    else:\n",
    "        return np.zeros(np.array(vlist).shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653abeaa021c4f4a8ee4d6511311b188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac5b5c2819a4f8fa29ed2114244693f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1471406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "possible_aggfuncs = [\"idf\"\n",
    "                     #\"mean\", \n",
    "                     #\"inverse_count\"\n",
    "                    ]\n",
    "\n",
    "aggregated_doc_vectors = {}\n",
    "\n",
    "# Aggregate vectors of documents beforehand\n",
    "for aggfunc in tqdm(possible_aggfuncs):\n",
    "    aggregated_doc_vectors[aggfunc] = np.zeros((len(preprocessed_documents), word_embeddings.shape[1]))\n",
    "    for index, doc in tqdm(enumerate(preprocessed_documents), total=len(preprocessed_documents)):\n",
    "        tokenized_doc = fasttext.tokenize(doc.strip())\n",
    "        vlist = [vector_dict[token] for token in tokenized_doc if token in vector_dict]\n",
    "        if aggfunc == 'idf':\n",
    "            weights = [idfs.get(word, 0) for word in tokenized_doc if word in vector_dict]\n",
    "        elif aggfunc == 'inverse_count':\n",
    "            counts = Counter(tokenized_doc)\n",
    "            weights = [1 / counts[word] for word in tokenized_doc if word in vector_dict]\n",
    "        if(len(vlist) < 1):\n",
    "            continue \n",
    "        else:\n",
    "            aggregated_doc_vectors[aggfunc][index] = aggregate_vector_list(vlist, aggfunc, weights) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n",
      "450\n"
     ]
    }
   ],
   "source": [
    "print(len(vector_dict['rovn\\u00c3\\u00a1'.lower()]))\n",
    "print(len(vector_dict['charlemagn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def aggregate_query(query, aggfunc, tokenized = False):\n",
    "    if not tokenized:\n",
    "        tokens = fasttext.tokenize(query.strip())\n",
    "    else:\n",
    "        tokens = query\n",
    "    if aggfunc == 'idf':\n",
    "        weights = []\n",
    "        vector = []\n",
    "        for word in tokens:\n",
    "            if word not in vector_dict:\n",
    "                word = synonyms.get(word, word)\n",
    "                if type(word) == list:\n",
    "                    for w in word:\n",
    "                        if w in vector_dict:\n",
    "                            weights.append(idfs.get(w, 0))\n",
    "                            vector.append(vector_dict[w])\n",
    "                else:\n",
    "                    if word in vector_dict:\n",
    "                        weights.append(idfs.get(word, 0))\n",
    "                        vector.append(vector_dict[word])\n",
    "            else:\n",
    "                weights.append(idfs.get(word, 0))\n",
    "                vector.append(vector_dict[word])\n",
    "        return aggregate_vector_list(vector, aggfunc, weights)\n",
    "    elif aggfunc == 'inverse_count':\n",
    "        counts = Counter(tokens)\n",
    "        weights = []\n",
    "        vector = []\n",
    "        for word in tokens:\n",
    "            if word not in vector_dict:\n",
    "                word = synonyms.get(word, word)\n",
    "                if type(word) == list:\n",
    "                    for w in word:\n",
    "                        if w in vector_dict:\n",
    "                            weights.append(1 / counts[w])\n",
    "                            vector.append(vector_dict[w])\n",
    "                else:\n",
    "                    if word in vector_dict:\n",
    "                        weights.append(1 / counts[word])\n",
    "                        vector.append(vector_dict[word])\n",
    "            else:\n",
    "                weights.append(1/ counts[word])\n",
    "                vector.append(vector_dict[word])\n",
    "        return aggregate_vector_list(vector, aggfunc, weights)\n",
    "    else:\n",
    "        return aggregate_vector_list([vector_dict[token] for token in tokens if token in vector_dict], aggfunc)\n",
    "    \n",
    "def get_most_similar_documents(query_vector, aggfunc, k = 5):\n",
    "    query_vector = query_vector.reshape(1, -1)\n",
    "    # Calculate the similarity with each vector. \n",
    "    # Hint: Cosine similarity function takes a matrix as input so you do not need to loop through each document vector.\n",
    "    sim = cosine_similarity(query_vector, aggregated_doc_vectors[aggfunc])\n",
    "    # Rank the document vectors according to their cosine similarity with \n",
    "    indexes = np.argsort(sim, axis=-1, kind='quicksort', order=None) # This is sorted in ascending order\n",
    "    indexes = indexes[0]\n",
    "    indexes = indexes[::-1] # Convert to descending\n",
    "    return indexes\n",
    "\n",
    "def search_vec_embeddings(query, topk = 10, aggfunc = 'mean', log=True):\n",
    "    query_vector = aggregate_query(query, aggfunc)\n",
    "    #print(\"Query vector shape: \", query_vector.shape)\n",
    "    indexes = get_most_similar_documents(query_vector, aggfunc)\n",
    "    # Print the top k documents\n",
    "    indexes = indexes[0:topk]\n",
    "    if log:\n",
    "        for index in indexes:\n",
    "            print(f'Document id: {corpus_df.iloc[index].name}')\n",
    "            print(original_documents[index])\n",
    "            print()\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess all queries in the queries_df\n",
    "\n",
    "queries = queries_df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'is', 'the', 'gatewai']\n",
      "['in', 'recent', 'year', 'new', 'chapter', 'becam', 'inevit', 'for', 'the', 'stori', 'statu', 'and', 'it', 'park', 'that', 'had', 'long', 'serv', 'as', 'a', 'sort', 'of', 'gatewai', 'from', 'south', 'scranton', 'onto', 'the', 'massiv', 'landmark', 'harrison', 'avenu', 'bridg', 'that', 'is', 'becaus', 'the', 'state', 'depart', 'of', 'transport', 'is', 'replac', 'the', 'old', 'crumbl', 'bridg']\n",
      "['first', 'call']\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "# remove the word 'call' from stopwords and return the list of stopwords\n",
    "my_stop_words = STOPWORDS.difference({'call', 'first'})\n",
    "\n",
    "test = \"what is the gateway\"\n",
    "test2 = \"In recent years, new chapters became inevitable for the storied statue and its park that had long-served as a sort of gateway from South Scranton onto the massive, landmark Harrison Avenue Bridge. That is because the state Department of Transportation is replacing the old, crumbling bridge.\"\n",
    "test3 = 'when were the first call'\n",
    "test4 = '3/5 of 60'\n",
    "CUSTOM_FILTERS2 = [lambda x: x.lower(), strip_tags, strip_punctuation, lambda x: strip_short(s=x,minsize=1), strip_multiple_whitespaces, lambda x: remove_stopwords(x, stopwords=my_stop_words), stem_text]\n",
    "print(preprocess_string(test, CUSTOM_FILTERS))\n",
    "print(preprocess_string(test2, CUSTOM_FILTERS))\n",
    "print(preprocess_string(test3, CUSTOM_FILTERS2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf250e6f57b4590ae3b2df4e227d3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing queries:   0%|          | 0/509962 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CUSTOM_FILTERS2 = [lambda x: x.lower(), strip_tags, strip_punctuation, lambda x: strip_short(s=x,minsize=1), strip_multiple_whitespaces]\n",
    "\n",
    "queries = [preprocess_string(query, CUSTOM_FILTERS) for query in tqdm(queries, desc='Preprocessing queries', total=len(queries))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df['text'] = queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5309e03a5404699a2712474ef4ea814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7437 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# iterate over all the querys in test data\n",
    "\n",
    "task1_matrix = np.zeros((len(test_data), word_embeddings.shape[1]))\n",
    "\n",
    "for index, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
    "    query = queries_df.loc[row['query_id']]['text']\n",
    "    #print(index, 'got query', query, index)\n",
    "    query_vector = aggregate_query(query, 'idf', True)\n",
    "    #print(query, len(query_vector))\n",
    "    task1_matrix[index] = query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NearestNeighbors(algorithm=&#x27;brute&#x27;, metric=&#x27;cosine&#x27;, n_neighbors=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NearestNeighbors</label><div class=\"sk-toggleable__content\"><pre>NearestNeighbors(algorithm=&#x27;brute&#x27;, metric=&#x27;cosine&#x27;, n_neighbors=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "NearestNeighbors(algorithm='brute', metric='cosine', n_neighbors=10)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Create a nearest neighbor model using cosine similarity\n",
    "# We'll use this to find the 10 most similar documents\n",
    "nn = NearestNeighbors(n_neighbors=10, metric='cosine', algorithm='brute')\n",
    "\n",
    "nn.fit(aggregated_doc_vectors['idf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, indices = nn.kneighbors(task1_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_results = indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the results to a pickle file\n",
    "with open(f'{DATA_DIR}/task1_results.pickle', 'wb') as f:\n",
    "    pickle.dump(task1_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load task2 test data\n",
    "test_data2 = pd.read_csv(f'{DATA_DIR}/task2_test.tsv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text    Anti-Money Laundering (AML) Source Tool for Br...\n",
       "Name: 1036904, dtype: object"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df.loc[1036904]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text    Anti-Money Laundering (AML) Source Tool for Br...\n",
       "Name: 1036904, dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df.iloc[1460658]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c29da205c3429ea5445591898fa79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# iterate row by row\n",
    "task2_results = []\n",
    "for index, row in tqdm(test_data2.iterrows(), total=len(test_data2)):\n",
    "    query = queries_df.loc[row['query-id']]['text']\n",
    "    query_vector = aggregate_query(query, 'idf', True)\n",
    "    #print(query, len(query_vector))\n",
    "    sim = cosine_similarity(query_vector.reshape(1, -1), aggregated_doc_vectors['idf'])\n",
    "    scores = []\n",
    "    for corpus_id in eval(row['corpus-id']):\n",
    "        # get the row index of corpus_id\n",
    "        corpus_index = corpus_df.index.get_loc(corpus_id)\n",
    "        scores.append(sim[0][corpus_index])\n",
    "    task2_results.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results to a pickle file\n",
    "with open(f'{DATA_DIR}/task2_results.pickle', 'wb') as f:\n",
    "    pickle.dump(task2_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a csv file for submission\n",
    "# HEADER: id,corpus-id,score\n",
    "# task1 results will be: query-id,[corpus-id1, corpus-id2, ...] (top 10 corpus-ids), -1\n",
    "# task2 results will be query-id, -1, [score1, score2...] \n",
    "# create the file\n",
    "\n",
    "with open(f'{DATA_DIR}/word2vec_submission.csv', 'w') as f:\n",
    "    id = 0\n",
    "    f.writelines('id,corpus-id,score\\n')\n",
    "    for i, row in enumerate(task1_results):\n",
    "        # convert row into a str\n",
    "        to_write = \"\\\"[\"\n",
    "        for j, corpus_index in enumerate(row):\n",
    "            # get corpus id from corpus_index\n",
    "            corpus_id = corpus_df.iloc[corpus_index].name\n",
    "            if j != len(row)-1:\n",
    "                to_write += str(corpus_id) + \", \"\n",
    "            else:\n",
    "                to_write += str(corpus_id)\n",
    "        to_write += \"]\\\"\"\n",
    "        f.write(str(id) + \",\" + to_write + \",-1\\n\")\n",
    "        id += 1\n",
    "\n",
    "    for i,row in enumerate(task2_results):\n",
    "        query_id_to_write = test_data2.iloc[i]['query-id']\n",
    "        to_write = \"\\\"[\"\n",
    "        for j, score in enumerate(row):\n",
    "            if j != len(row)-1:\n",
    "                to_write += str(score) + \", \"\n",
    "            else:\n",
    "                to_write += str(score) \n",
    "        to_write += \"]\\\"\"\n",
    "        f.write(str(id) + \",-1,\" + to_write + \"\\n\")\n",
    "        id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
